\documentclass[12pt]{article}

\newcommand{\documentname}{\textsl{Note}}
\newcommand{\equationname}{equation}
\newcommand{\foreign}[1]{\textsl{#1}}
\newcommand{\etal}{\foreign{et\,al.}}
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\given}{\,|\,}
\newcommand{\setofall}[1]{\left\{{#1}\right\}}
\newcommand{\dd}{\mathrm{d}}

\begin{document}\sloppy\sloppypar

\noindent{\ttfamily This document is a draft.  It is Copyright 2012
  David W. Hogg, Ross Fadely, Rob Fergus, and others.  It is not ready
  for distribution.}

\begin{abstract}
In astronomical imaging projects, pixel-level calibration (bias, dark,
and flat) estimated via zero-length exposures and images of an
illuminated dome or twilight sky may not be optimal for application to
the measurements of greatest scientific interest.  Furthermore, in
most present-day experiments, many more photons are collected in the
scientific object data than in the calibration data.  For these
reasons, it makes sense to ask whether the full set of calibration
information could be derived from the science data alone.  In this
\documentname, we build very flexible models of astronomical imaging
based on mixtures of Gaussians (really mixtures of factor analyzers)
and use those models to test and improve pixel-level calibration
parameters.  We demonstrate with real data from the \project{Sloan
  Digital Sky Survey} and the \project{Hubble Space Telescope
  Wide-Field Camera 3} that we can accurately determine calibration
information using only science data.  Our self-calibration method is
computationally expensive, but has the advantages that (by
construction) it infers the parameters that are directly relevant to
the science data, that it captures the large amount of calibration
information latent in the full scientific data set, it puts almost no
requirements on repeat imaging or other aspects of survey design, and
that (of course) it reduces or obviates calibration overheads.
\end{abstract}

\section{Introduction}

...on photometric self-calibration, cite Padmanabhan \etal~20xx, Holmes
\etal~2012.  Note limitation that these methods work at catalog level,
don't calibrate below the resolution of the photometric methodology,
and aren't sensitive to additive problems.

...Notes about flexible models, maybe even citing a previous paper by
Fadely?  It is not clear to Hogg whether these should be separate
papers or the same paper.

\section{Image patch model}

...Hogg proposes that we be very consistent with indexing.  I propose
that we have $N$ $d$-pixel (say $d=49$ or $81$) image patches $n$ on
which we train the model.  The model will have $K$ Gaussian components
$k$, each of which is a mixture of factor analyzers with $M\ll d$
factors $m$.  The imaging detector will have $J$ independent pixels
(or, in the case of \project{SDSS}, pixel columns) $j$.  The detector
has been used to take $H$ exposures $h$, each of which has (we hope) a
different exposure time $t_h$ and has seen (we hope) a different
astronomical scene...

...We need an astronomer-friendly explanation of mixture of factor
analyzers.  Hogg is at your service...

...We need to give the posterior prediction of one pixel's intensity
given all the neighbors' intensities...

\section{Calibration parameter inference}

...something like: For every pixel position $j$ in the detector, there
are calibration parameters $\theta_j$ (including possibly a zero level
$z_j$, a dark current rate $d_j$, and a sensitivity $f_j$).  Detector
pixel $j$ also has $7\times 7-1=48$ (fewer if it is an edge pixel)
neighbor pixels $\ell$.  In the raw data, in exposure $h$...

...We are going to assume that our MFA model is a good approximation
to a prior PDF over possible properly calibrated (though noisy)
intensities.  That is, we will assume that the calibration parameters
are \emph{close to correct}...

...likelihood of the parameters given the data---counts $C_{jh}$
coming from pixel $j$ in exposure $h$---is something like
\begin{eqnarray}
p(C_{jh}\given t_h,\theta_j,\alpha) &=& \int_0^\infty \delta(C_{jh} - c_{jh})\,p(I_{jh}\given\setofall{I_{\ell h}}_{\ell=1}^L,\alpha)\,\dd I_{jh}
\label{eq:calibration}\\
\theta_j &\equiv& \setofall{z_j, d_j, f_j, \cdots}
\\
\alpha &\equiv& \setofall{a_k,m_k,V_k}_{k=1}^K
\\
c_{jh} &=& z_j + d_j\,t_h + f_j\,t_h\,I_{jh} + \cdots
\end{eqnarray}
where $t_h$ is the exposure time for exposure $h$, $\theta_j$ are the
calibration parameters for pixel $j$, $\alpha$ is the full parameter
blob for the MFA model, the integral is over $I_{jh}$, the (true,
pixel-convolved PSF-convolved) intensity falling on pixel $j$ in
exposure $h$, $\delta(\cdot)$ is the delta function, $c_{jh}$ is the
predicted counts given the intensity and exposure time and pixel
calibration parameters, $p(I_{jh}\given\cdot)$ is the probability of
getting $I_{jh}$ given the intensities $I_{\ell h}$ of all the
neighbor pixels $\ell$ in exposure $h$ and the MFA parameter blob
$\alpha$.  The full expression for this posterior prediction is given
in \equationname~(??).

The likelihood for the calibration parameters $\theta_j$ given in
\equationname~(\ref{eq:calibration}) is slightly odd in that it treats
the data as being generated deterministically from the intensity
(there is a delta function).  The expression is deterministic rather
than probabilistic because in the MFA model, the noise in the data is
implicitly included in the variance tensors of the mixture components.
In the (common) case that an accurate noise model for the data is
known, the MFA model could be replaced with a factor-analysis
generalization of the \project{extreme deconvolution} method (Bovy,
Hogg \& Roweis 20xx).  This would presumably make the calibration more
accurate (because it would be incorporating more prior information)
but at the expense of generality (and, we expect, computer time).

\section{Data and results}

\section{Discussion}

...come back to the \emph{known noise model} issue...

\end{document}
