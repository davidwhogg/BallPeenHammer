% This file is part of the BallPeenHammer project.
% Copyright 2013, 2014 the authors.

\documentclass[12pt,letterpaper,preprint]{aastex}

\newcommand{\instrument}[1]{\textsl{#1}}
\newcommand{\rf}[1]{\textbf{RF: #1}}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\HST}{\instrument{HST}}
\newcommand{\WFC}{\instrument{WFC3}}
\newcommand{\FLT}{\texttt{FLT}\,\,}
\newcommand{\data}{\vect{D}}
\newcommand{\model}{\vect{M}}
\newcommand{\var}{\vect{\sigma}^2}
\newcommand{\gradients}{\vect{g}}

\begin{document}

\title{The \HST\ \WFC\ IR-channel point-spread function}
\author{Ross Fadely \& David W. Hogg}

\begin{abstract}
Accurate point-source photometry and astrometry depends on an accurate model of the point-spread function (PSF).
The publicly available models for the PSF in the \HST\ \WFC\ IR-channel instrument are not accurate.
Here we construct an empirical model of this PSF,
  using archival observations of compact sources taken with the instrument.
Our model is of the \emph{pixel-convolved} PSF;
  it is a model for the PSF as it appears in the pixel read-outs of the instrument
  as a function of the sub-pixel position of the source.
We build our model for the XXX, YYY, and ZZZ bandpasses;
  the PSF in other bandpasses can be interpolated from these.
The variation of the PSF with position in the focal plane
  is apparent in the model.
We don't explicitly model the dependence of the PSF on source color,
  but we show that there are effects evident in the data.
All the code is released under an open-source license
  and all the results are presented in machine-readable form.
\end{abstract}

\keywords{
  method:~statistical
}

\section{Introduction}

The imaging devices on the \instrument{Hubble Space Telescope} (\HST) are generally not well sampled,
  meaning that they do not have multiple pixels spanning the full-width half-maximum (FWHM)
  of the point-spread function (PSF).
In a space-based mission,
  good sampling is traded off against detector size, field-of-view, and telemetry bandwidth.
Sampling is often made lower in priority,
  especially since well-sampled images can be made by taking multiple
  ill-sampled images at partial-pixel dithers.
Poor sampling renders some image operations,
  like pixel interpolation, point-source astrometry, and identification of cosmic rays,
  substantially more difficult than they are in well-sampled images.

However, even if sampling is poor,
  the response of the device to a point source is nonetheless continuous in source position.
That is, it is possible, even for poorly sampled images,
  to make a prediction or model of the pixel brightnesses generated by
  a star of any brightness at any (precise, sub-pixel) position in the focal plane.
This prediction gets smoother as the sampling is improved,
  and the model gets less complex (less featured),
  but it can be made at any sampling, even very bad samplings.
There is nothing difficult in principle about making this prediction or model,
  nor in training it with real imaging of real stars taken with the device.
What's astonishing is that,
  for the incredibly productive and important \HST\ imaging instruments,
  this model has never (to our knowledge) been made (soften this?).
Here we address this by building, testing, and releasing such a model.

There are models of the \emph{optical PSF} of the \HST\ (cite TinyTim, anderson06);
  these are models of the light field falling on the detector pixels. \rf{there are WFC3 IR psfs build by Anderson and Dolphin}
The optical PSF is not directly observed in an image;
  what is observed is the pixel-convolved PSF---%
  the optical PSF integrated over a pixel---%
  which, when the imaging is poorly sampled,
  depends strongly on both the optical PSF
  and the detailed shape and sensitivity of the pixels.
What a data analyst wants is a model of the pixel-convolved PSF,
  which can be used directly to perform astrometric and photometric measurements on images,
  without additional assumptions about the pixels.
In addition to this ``unobservability'' of the optical PSF,
  there is the additional problem that the aforementioned optical PSF models
  are built not from the (abundant) data expensively telemetered down from the Satellite,
  but rather from theoretical models of the complicated optics.
We find that these optical PSF models are not only \emph{not} what we need in our data analyses,
  they are also inaccurate, probably because they haven't been forced to agree with the data.

In what follows we build an empirical or data-driven model
  of the pixel-convolved PSF for the \HST\ \WFC\ IR-channel imaging device.
Our model is novel in several respects:
It is based entirely on data;
  it is a flexible model of real data.
It is built using essentially all the available data from the instrument.
It accounts for dependences of the PSF on bandpass and focal-plane position.
It is a pixel-convolved PSF,
  so it predicts the pixel read-outs very directly and without convolution by any pixel model.
It extremely accurately predicts the pixel contents of real images.
It is built with open-source code,
  all of which is available for repurposing to other imagers on \HST\ or other space-based facilities.

\section{Data}

\section{Method}

\subsection{Model}

The MAST pipeline delivers an error estimate for each pixel in the \FLT images.   When optimizing 
our model we would like to use this error in order to calculate quantities like $\chi^2$.  However, 
there is an issue preventing us from doing so.  For typical astronomical detectors, the variance of 
the data is estimated as a constant (read noise-like term) plus a scaler times the intensity of the pixel 
(gain-like term).  This means a noisy pixel will have an under or over estimated uncertainty depending 
on whether the pixel value is below or above the true value.  This has been shown to be 
problematic in the PSF fitting of SDSS data (cite Blanton??).  A better model for the uncertainty is 
instead one which scales with the value of the \emph{model}, since we hope the model value is a 
closer estimate of the true pixel intensity.  We therefore adopt a noise model for each patch $\data_i$
as:

\begin{eqnarray}
\var_i =  \sigma_{r}^2 + g \model_i
\quad , 
\label{eqn:model-noflat}
\end{eqnarray}

\noindent where $\sigma_{r}^2$ is `read noise-like' constant scalar and $g$ is a `gain-like' scalar 
which is multiplied by the model.  For simplicity, we set each of these parameters to be the same 
for each data patch.  To set $g$, we took the MAST estimate for the squared error and 
divided by the pixel values.  We then fit a line to pixels with values between 100 and 
400 electrons per second, since such pixels are relatively unaffected by read noise.  We find that 
the slope of this line was consistent with a value of 
$\sim0.01$, but increased sharply for images with exposure time $\lesssim 200$ seconds.  
Nevertheless, we set $g=0.01$ and analyzed the consequences of doing so by examining the 
residuals.  For $\sigma_{r}^2$, we took the median of the MAST squared error estimates.  We find 
significant variation of the median from image to image, but simply set $\sigma_{r}^2=0.05$ since 
it comprises a small contribution to the model of $\var$ for the vast majority of the pixels in our 
patches.

\subsection{Optimization}

We optimize the loss function:
\begin{eqnarray}
L =  \frac{1}{2}\sum_i\left(\ln(\var_i) + \sum_{i} \frac{\data_i - \model_i}{\var_i} + C\right) + 
\epsilon \sum \gradients^2 
\quad .
\label{eqn:model-noflat}
\end{eqnarray}

\noindent The first term of this loss function is simply the negative log likelihood of the data, 
when the constant $C$ is set to $\ln(2\pi)$.  The second term is a smoothness regularization which 
we take to the gradient at pixel in the pixel-convolved PSF model times a regularization 
strength $\epsilon$.

The goal of our optimization is to find a very good model for each patch $\model_i$, which should 
necessitate a good model for the \WFC PSF.  To do so we must solve for $\{x_i, y_i, A_i, \bkg_i\}$ 
and the pixel-convolved PSF model.  We take an iterative approach to this optimization, relying on  
a modified Powell optimization algorithm (implemented in \texttt{scipy.optimize.fmin_Powell}). 

\section{Results}

\section{Discussion}

We perform very well.

We are the only publicly available PSF and software!
(Maybe here or in an appendix give some example command-line operation.)

It would be good to have more data;
  what could we do if we did?

The problem is sparse in structure;
  we could have made much more use of that sparsity to speed things up.

We had various knobs to turn; we set them heuristically.
What would be better?

We worked in a very knowledge-free way;
  we could have used our prior knowledge better.
For instance, we could have started by subtracting out the pixel-convolved fourier transform of the entrance aperture;
  then we would only have been fitting residuals.
Even better would be to start with a prior PDF over PSFs;
  that's a dream.

We only fit the pixel-convolved PSF.
Once again, why is this a good idea?
Once again, how should this be used by other users?
How could one infer the optical PSF from these results?

\end{document}
